---
title: "TPQ_R_Python"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = False}
options(device = "X11")

library(reticulate)
if (dir.exists("/home/abdulrahman/anaconda3/envs/mne/bin/")){
  use_python ("/home/abdulrahman/anaconda3/envs/mne/bin/python3")
}else{
  use_python ("/home/asawalma/anaconda3/envs/mne/bin/python")
}

library(ggplot2)
library(psy)
```

```{python Is the combined-subject IC composition determined mainly by HCs}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

path = '/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/Palestine/ICASSO/icasso_tpq_reco/bootstrap/'

fname_reco_all = path + 'icasso_HC,MDD,PTSD,TNP,GAD_nsamp1822_n_comp13_n_iter100_dist0.50_bootstrap_MNEinfomax_data_reco.npz'
fname_reco_hc = path + 'icasso_HC_nsamp1202_n_comp13_n_iter100_dist0.50_bootstrap_MNEinfomax_data_reco.npz'
fname_reco_mdd = path + 'icasso_MDD_nsamp455_n_comp06_n_iter100_dist0.45_bootstrap_MNEinfomax_data_reco.npz'
ids_all = path + "../ids_all_sess1_2.npy"
# load data
npz_all = np.load(fname_reco_all, allow_pickle=True)
npz_hc = np.load(fname_reco_hc, allow_pickle=True)
npz_mdd = np.load(fname_reco_mdd, allow_pickle=True)
npz_ids = np.load(ids_all, allow_pickle=True)

#rearrange groups and IDs according to the IDs in npz_ids
df = pd.read_excel("/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/TPQ_DataAndAnalysis/TPQData_ICA_25.11.2020.xlsx")
sub_inds = [np.where(df["ID"] == npz_ids[i])[0][0] for i in range(len(npz_ids))]
groups = [df["Group"][item] for item in sub_inds]
IDs = [df["ID"][item] for item in sub_inds]
session = [df["Session"][item] for item in sub_inds]
#create info_array which contains groups, IDs and session
info_array = np.array([[groups[i], IDs[i], session[i]] for i in range(len(IDs))])
print(info_array)


average_all = np.average(npz_all['data_reco'],axis = 1)
average_hc = np.average(npz_hc['data_reco'],axis = 1)
average_mdd = np.average(npz_mdd['data_reco'],axis = 1)


pd.DataFrame(np.transpose(average_all)).to_csv(path+"npz_all_avg.csv", index=False)
pd.DataFrame(np.transpose(average_hc)).to_csv(path+"npz_hc_avg.csv", index=False)
pd.DataFrame(np.transpose(average_mdd)).to_csv(path+"npz_mdd_avg.csv", index=False)

# create data frames for each of the ICs and save it
for i in range(npz_all['data_reco'].shape[0]):
    IC_all = npz_all['data_reco'][i,:,:]

    # add info arrays to data frames
    IC_all = np.append(info_array,IC_all,axis = 1)

    # convert to data frame
    IC_all = pd.DataFrame(IC_all)

    #change column names
    colnames = ["Group","ID","Session"]
    colnames+=list(range(1,101))
    IC_all.columns = colnames

    #save to csv
    IC_all.to_csv(f"{path}IC_arrays/npz_all_{i}.csv", index=False)

for i in range(npz_mdd['data_reco'].shape[0]):
    IC_mdd = npz_mdd['data_reco'][i,:,:]
    pd.DataFrame(IC_mdd).to_csv(f"{path}IC_arrays/npz_mdd_{i}.csv", index=False)

for i in range(npz_hc['data_reco'].shape[0]):
    IC_hc = npz_hc['data_reco'][i,:,:]
    pd.DataFrame(IC_hc).to_csv(f"{path}IC_arrays/npz_hc_{i}.csv", index=False)


```

```{r  find c.alpha for MDD and HC (from within the IC_all)}
#### 2. find c.alpha for MDD and HC (from within the IC_all)                   #
#### and find the correlation between IC_mdd with IC_all, and IC_hc with IC_all#
#### This will be done for the last component (supposedly the combined IC)     #####
IC_Num_all <- 4
IC_Num_hc <- 4
IC_Num_mdd <- 2

df_all = read.csv(paste0(path, "IC_arrays/npz_all_",IC_Num_all,".csv"))
df_hc = read.csv(paste0(path, "IC_arrays/npz_hc_",IC_Num_hc,".csv"))
df_mdd = read.csv(paste0(path, "IC_arrays/npz_mdd_",IC_Num_mdd,".csv"))




cr_list_hc = vector()
for (i in 1:100){
  question_df = data.frame(df_all[1:1202,i+3],df_hc[,i])
  colnames(question_df) = c(paste0("All_IC",IC_Num_all,"_Q",i),paste0("HC_IC",IC_Num_hc,"_Q",i))
  cr_list_hc = append(cr_list_hc, cronbach(question_df)$alpha)
}


cr_list_mdd = vector()
for (i in 1:100){
  question_df = data.frame(df_all[df_all$Group=="MDD",i+3],df_mdd[,c(i)])
  colnames(question_df) = c(paste0("All_IC",IC_Num_all,"_Q",i),paste0("HC_IC",IC_Num_mdd,"_Q",i))
  cr_list_mdd = append(cr_list_mdd, cronbach(question_df)$alpha)
}



#exclude those above 1 or below -1
cr_list_hc = cr_list_hc[cr_list_hc>-1 & cr_list_hc<1]
cr_list_mdd = cr_list_mdd[cr_list_mdd>-1 & cr_list_mdd<1]

mdd_mean_a = mean(abs(cr_list_mdd))
hc_mean_a = mean(abs(cr_list_hc))

Correlate(combined_df,"All","HC")
Correlate(combined_df,"All","MDD")
Correlate(combined_df,"HC","MDD")

# HC are correlated with IC_all for the same subjects, MDD are less so
################################################################################
# Result:                                                                      #
# c.alpha average for the 100 questions is higher for HC                       #
################################################################################
```

```{r, warning=FALSE}
library (ppcor)
library(readxl)
#### 3. find c.alpha for MDD and HC for each component within their own  ICs   #

# create c.alpha for IC loadings and for patient scores
#determine col types, then load the large data set
#col types = 11 text, 17 numeruc, 1 text, 1 numeric, 6 text, 104 numeric and 100 text
colTypes = c(rep("text",11),rep("numeric",17),rep("text",1),rep("numeric",1),rep("text",6),rep("numeric",104),rep("text",100))
scores = read_excel("/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/TPQ_DataAndAnalysis/TPQ_Analysis_All_25.11.2020.xlsx", col_types = colTypes)

# convert 0 values to -1
for (i in 1:100){
  scores[paste0("Q",i)] = scores[paste0("Q",i)]*2-1
}


#load the loadings
loadings_path = paste0(path,"/IC_arrays/","npz_all_",0,".csv")
loadings = read.csv(loadings_path)


#define a function to find the item position, to be used in "sapply" below
m_fun <- function(x){
  match(x, scores$`Final ID`)
}
HC_ids = loadings$ID[loadings$Group == "HC"]
MDD_ids = loadings$ID[loadings$Group == "MDD"]
HC_inds = sapply(HC_ids, m_fun)
MDD_inds = sapply(MDD_ids, m_fun)


weighted_scores_HC = scores[HC_inds,match("Q1", colnames(scores)):match("Q100", colnames(scores))]
for (i in 1:100){
  weighted_scores_HC[paste0("Q",i)] = scores[HC_inds,paste0("Q",i)] * loadings[loadings$Group == "HC",paste0("X",i)]
}

weighted_scores_MDD = scores[MDD_inds,match("Q1", colnames(scores)):match("Q100", colnames(scores))]
for (i in 1:100){
  weighted_scores_MDD[paste0("Q",i)] = scores[MDD_inds,paste0("Q",i)] * loadings[loadings$Group == "MDD",paste0("X",i)]
}

cronbach(weighted_scores_MDD[,-c(61,71)])
cronbach(weighted_scores_HC[,-c(61,71)])
cronbach(loadings[loadings$Group == "MDD",])

################################################################################
# Result:                                                                      #
# The worst cronbach anyone would imagine                                      #
################################################################################
```

```{python Fast_ICA difference between bootstrapped and non-boot strapped}
import numpy as np
path = '/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/Palestine/ICASSO/icasso_tpq_reco/bootstrap/'
# Next issue: does Fast ICA differ between bootstraping and no bootstraping?
path_n_bootstrap = path + "../no_bootstrap/icasso_HC_nsamp1202_n_comp13_n_iter100_dist0.40_no-bootstrap_MNEfastica_data_reco.npz"
path_bootstrap = path + "icasso_HC_nsamp1202_n_comp13_n_iter100_dist0.50_bootstrap_MNEfastica_data_reco.npz"

npz_boot = np.load(path_bootstrap, allow_pickle=True)
npz_n_boot = np.load(path_n_bootstrap, allow_pickle=True)

# convert to list of data frames so that R can read it
data_boot = [item for item in npz_boot['data_reco']]
data_n_boot = [item for item in npz_n_boot['data_reco']]
```

```{r check the difference in FastICA between bootstrapping and no bootstrapping, warning=FALSE}
colTypes = c(rep("text",11),rep("numeric",17),rep("text",1),rep("numeric",1),rep("text",6),rep("numeric",104),rep("text",100))
scores = read_excel("/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/TPQ_DataAndAnalysis/TPQ_Analysis_All_25.11.2020.xlsx", col_types = colTypes)
scores = as.data.frame(scores)

# convert T to 1 and F to -1, also convert "NA" to NA
for (i in 1:100){
  scores[scores[,paste0("QO",i)] =="NA",paste0("QO",i)] = NA
  scores[!is.na(scores[paste0("QO",i)]),paste0("QO",i)] = ifelse(scores[!is.na(scores[paste0("QO",i)]),paste0("QO",i)]=="T","1","-1")
  scores[,paste0("QO",i)] = as.numeric(scores[,paste0("QO",i)])
}


data_boot = py$data_boot
data_n_boot = py$data_n_boot
IC_df = data.frame(data_n_boot[1])

weighted_df = scores[HC_inds, match("QO1", colnames(scores)):match("QO100", colnames(scores))]
for (i in 1:100){
  weighted_df[paste0("QO",i)] = scores[HC_inds,paste0("QO",i)] * IC_df[paste0("X",i)]
}


# all correlations are either +1 or -1 with a p-value of 0
# let's divide the questions to +vely correlated and -vely correlated
matrix = cor(IC_df)
positive_correlates = colnames(IC_df) %in% rownames(data.frame(matrix[1,][matrix[1,]>0]))
negative_correlates = colnames(IC_df) %in% rownames(data.frame(matrix[1,][matrix[1,]<0]))

#only take those with absolute value of >0.25
averages = apply(IC_df,2,mean)
inclusion_25 = abs(averages)>0.25

#Also, notice that there are columns that are +ve and others that are -ve
# first, find the averages
averages = apply(IC_df,2,mean)
positive_values = averages>0.25
negative_values = averages < -0.25


TPQ.pca = princomp(na.omit(weighted_df))
factor_analysis = factanal(na.omit(weighted_df), factors = 4, lower=0.3)
abs(factor_analysis$loadings)>0.3
#now calculate cronbach alpha according to loadings' correlation
cronbach(weighted_df[positive_correlates&inclusion_25])
cronbach(weighted_df[negative_correlates&inclusion_25])

#now calculate cronbach alpha according to loadings' values
cronbach(weighted_df[positive_values])
cronbach(weighted_df[negative_values])

cronbach(weighted_df)


```

```{python import all data to check the difference between Fast_ICA vs Infomax}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

###############################################################
#                     VERY IMPORTANT                          #
# make sure that you use data frames of pandas and not numpy, #
# otherwise they will be converted to faulty matrices         #
###############################################################



path = '/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/Palestine/ICASSO/icasso_tpq_reco/bootstrap/'
if not os.path.exists(path):
  path = path.replace("/abdulrahman/abdulrahman.sawalma@gmail.com","/asawalma/Insync/abdulrahman.sawalma@gmail.com/Google Drive")
path_infomax = path + "icasso_HC,MDD,PTSD,TNP,GAD_nsamp1822_n_comp13_n_iter100_dist0.50_bootstrap_MNEinfomax_data_reco.npz"
path_fastica = path + "icasso_HC,MDD,PTSD,TNP,GAD_nsamp1822_n_comp13_n_iter100_dist0.50_bootstrap_MNEfastica_data_reco.npz"
path_noboot_fastica = path + "../no_bootstrap/icasso_HC,MDD,PTSD,TNP,GAD_nsamp1822_n_comp13_n_iter100_dist0.40_no-bootstrap_MNEfastica_data_reco.npz"
path_ids = path + "../ids_all_sess1_2.npy"

npz_info = np.load(path_infomax, allow_pickle=True)
npz_fast = np.load(path_fastica, allow_pickle=True)
npz_noboot_fast = np.load(path_noboot_fastica, allow_pickle=True)
npz_ids = np.load(path_ids, allow_pickle=True)
npz_ids = [[item] for item in npz_ids]

# convert to list of data frames so that R can read it
# I also added the IDs for each frame using np.append
data_info = [pd.DataFrame(np.append(npz_ids,item, axis = 1)) for item in npz_info['data_reco']]
data_fast = [pd.DataFrame(np.append(npz_ids,item, axis = 1)) for item in npz_fast['data_reco']]
data_noboot_fast = [pd.DataFrame(np.append(npz_ids,item, axis = 1)) for item in npz_noboot_fast['data_reco']]
data_noboot_fast[0]
````

```{r load all data for further analysis , warning=FALSE, cache=TRUE}
library (ppcor)
library(readxl)
library(reshape2)
library(ggplot2)
library(dplyr)
library(Hmisc)



# determine coltypes before reading data
colTypes = c(
  rep("text", 11),
  rep("numeric", 17),
  rep("text", 1),
  rep("numeric", 1),
  rep("text", 6),
  rep("numeric", 104),
  rep("text", 100)
)


#read the data
score_parent = "/home/abdulrahman/abdulrahman.sawalma@gmail.com/PhD/Data/TPQ_DataAndAnalysis/"
score_path = paste0(score_parent, "TPQ_Analysis_All_25.11.2020.xlsx")
if (!dir.exists(score_parent)) {
  score_path = gsub(
    "/abdulrahman/abdulrahman.sawalma@gmail.com",
    "/asawalma/Insync/abdulrahman.sawalma@gmail.com/Google Drive",
    score_path
  )
}

scores = read_excel(score_path, col_types = colTypes)


#convert the tibble object (read by readxl) to ordinary data frame
scores = as.data.frame(scores)


# convert T to 1 and F to -1, also convert "NA" to NA
for (i in 1:100) {
  scores[scores[, paste0("QO", i)] == "NA", paste0("QO", i)] = NA
  scores[!is.na(scores[paste0("QO", i)]), paste0("QO", i)] = ifelse(scores[!is.na(scores[paste0("QO", i)]), paste0("QO", i)] ==
                                                                      "T", "1", "-1")
  scores[, paste0("QO", i)] = as.numeric(scores[, paste0("QO", i)])
}



# Make sure that 'scores' and 'loadings' data frames have the same arrangements
loadings_IDs = data.frame(py$data_fast[1])$X0
scores$arrangement = sapply(scores$`Final ID`, function(x)
  match(x, loadings_IDs))
scores = arrange(scores, arrangement)
scores = scores[!is.na(scores$arrangement), ]




prepare_data <- function(data_frame) {
  # formats the data to have these columns ID, Diagnosis, 100 loadings
  # also, makes sure that the arrangement in these data frames is the same as the data frame "scores"
  
  # column names variable
  question_names = sapply(1:100, function(x)
    paste0("Q", x, "_load"))
  ColNames = append("ID", question_names)
  
  # import python data as data frames
  colnames(data_frame) = ColNames
  
  # prepare diagnosis column
  Diagnosis = scores$Diagnosis[!is.na(sapply(scores$`Final ID`, function(x)
    match(x, data_frame$ID)))]
  
  # convert data to numeric
  data_frame[-1] = apply(data_frame[-1], 2, as.numeric)
  
  data_frame$arrangement = sapply(data_frame$ID, function(x)
    match(x, scores$`Final ID`))
  data_frame = arrange(data_frame, arrangement)
  data_frame = data_frame[!is.na(data_frame$arrangement), ]
  data_frame$Diagnosis = Diagnosis
  
  return (data_frame)
}


create_weights <- function(data_frame) {
  # a function to calculate the weights for all questions
  # returns a data frame of weighted values
  # start with a data frame by copying "scores", then apply everything on it
  weighted_df = scores[match("QO1", colnames(scores)):match("QO100", colnames(scores))]
  
  # calculate the loadings
  for (k in 1:100) {
    weighted_df[paste0("QO", k)] = scores[paste0("QO", k)] * data_frame[paste0("Q", k, "_load")]
  }
  weighted_df[c("ID", "Diagnosis")] = data_frame[c("ID", "Diagnosis")]
  
  return(weighted_df)
}


# make 3 lists for python data, prepare these data then add them to the lists
data_info = lapply(1:length(py$data_info), function(x)
  prepare_data(data.frame(py$data_info[x])))
data_fast = lapply(1:length(py$data_fast), function(x)
  prepare_data(data.frame(py$data_fast[x])))
data_noboot_fast = lapply(1:length(py$data_noboot_fast), function(x)
  prepare_data(data.frame(py$data_noboot_fast[x])))


# calculate the weights of all data frames
weighted_fast_list = lapply(1:length(data_fast), function(x)
  create_weights(data_fast[[x]]))
weighted_noboot_fast_list = lapply(1:length(data_noboot_fast), function(x)
  create_weights(data_noboot_fast[[x]]))
weighted_info_list = lapply(1:length(data_info), function(x)
  create_weights(data_info[[x]]))


bind_data_frames <- function(...) {
  # bind the columns two data frames, making sure to remove common columns
  # length of the passed arguments in "..."
  ar_len = length(match.call())-1
  
  #create a list of all data frames included
  if (ar_len == 1){
    all_data_list = c(...)
  } else{
    all_data_list = list(...)
    }
  
  #determine the colnames that are found in all data frames
  allcolnames = lapply(all_data_list, colnames)
  common_colnames = Reduce(intersect, allcolnames)
  
  # you need to be sure that all data frames contain the exact same common columns
  common_columns = all_data_list[[1]][common_colnames]
  
  #remove these columns from all data frames
  all_data_list = lapply(1:length(all_data_list), function(x)
    all_data_list[[x]][!colnames(all_data_list[[x]]) %in% common_colnames])
  
  # bind all data frames (together with the common columns)
  final_df = Reduce(bind_cols,list(common_columns,all_data_list))

  return(final_df)
}


df_fast_list = lapply(1:length(weighted_fast_list), function(x)
  bind_data_frames(weighted_fast_list[[x]], data_fast[[x]]))
df_noboot_fast_list = lapply(1:length(weighted_noboot_fast_list), function(x)
  bind_data_frames(weighted_noboot_fast_list[[x]], data_noboot_fast[[x]]))
df_info_list = lapply(1:length(weighted_info_list), function(x)
  bind_data_frames(weighted_info_list[[x]], data_info[[x]]))

```

```{r check percentage of explained variance, warning=FALSE}
# Now we check the explained variance
#create empty vectors for percentage of explained variance 
p_var_fast = vector()
p_var_noboot_fast = vector()
p_var_info = vector()

calc_var_perc <- function(data_frame_1,data_frame_2){
  # function to calculate percentage of explained variance
  # calculate explained variability percentage 
  var_perc = 100-100*mean(var(data_frame_1-data_frame_2,na.rm = TRUE),na.rm = FALSE)/mean(var(data_frame_2), na.rm = TRUE)
  return(var_perc)
}

p_var_fast = sapply(1:length(weighted_fast_list),function(x) calc_var_perc(weighted_fast_list[[x]][1:100], scores[141:240]))
p_var_noboot_fast = sapply(1:length(weighted_noboot_fast_list),function(x) calc_var_perc(weighted_noboot_fast_list[[x]][1:100], scores[141:240]))
p_var_info = sapply(1:length(weighted_info_list),function(x) calc_var_perc(weighted_info_list[[x]][1:100], scores[141:240]))


# plot the result
TypicalTheme=theme_bw(base_size = 16,base_family = "Times")+theme(panel.grid = element_blank(), plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5,face = "italic"))

Data = data.frame(id=1:14, FastICA_bootstrap = append(p_var_fast,rep(0,9)), FastICA_no_bootstap = p_var_noboot_fast, Infomax_bootstap = append(p_var_info,rep(0,9)))
Data = melt(Data,id.vars = "id",value.name = "p_var_explained", variable.name = "IC_Type")
ggplot(Data[Data$p_var_explained>0 & Data$IC_Type != "FastICA_no_bootstap",],mapping = aes(x=id, y= p_var_explained, fill = IC_Type, group = IC_Type))+
  geom_bar(stat="identity",position = position_dodge(0.9),color = "black")+
  scale_y_continuous(name = "Percentage of Explained Variance")+
  scale_x_continuous(name = "IC #")+
  scale_fill_manual(values = c("#Eb4C42","#4EA3DF","#6cBE58"))+
  TypicalTheme+
  ggtitle("% Explained Variance for Variable ICASSO Approaches")

ggplot(Data[Data$IC_Type != "Infomax_bootstap",],mapping = aes(x=id, y= p_var_explained, fill = IC_Type, group = IC_Type))+
  geom_bar(stat="identity",position = position_dodge(0.9),color = "black")+
  scale_y_continuous(name = "Percentage of Explained Variance")+
  scale_x_continuous(name = "IC #")+
  scale_fill_manual(values = c("#C33E3B","#6cBE58"))+
  TypicalTheme+
  ggtitle("% Explained Variance for Variable ICASSO Approaches")

calc_var_perc(weighted_fast_list[[4]][1:100],weighted_fast_list[[5]][1:100])
calc_var_perc(weighted_info_list[[1]][1:100],weighted_info_list[[5]][1:100])
calc_var_perc(weighted_noboot_fast_list[[4]][1:100],weighted_noboot_fast_list[[5]][1:100])

# percentage of explained variance is very different when we look at infomax
# I will have a look at the mean

sum_rows <- function (data_list,n){
  c_name = colnames(data_list[[1]])[n]
  ss = sapply(1:length(data_list), function(x) data_list[[x]][c_name])
  r_sum = list(rowSums(data.frame(ss),na.rm = TRUE)/length(data_list))
  names(r_sum) = c_name
  return(r_sum)
}

new_fast = data.frame(lapply(1:100, function (x) sum_rows(weighted_fast_list,x)))
new_noboot_fast = data.frame(lapply(1:100, function (x) sum_rows(weighted_noboot_fast_list,x)))
new_info = data.frame(lapply(1:100, function (x) sum_rows(weighted_info_list,x)))

calc_var_perc(new_info,scores[141:240])
```

```{r check the pure differences in the five ICs of FastICA and Infomax, warning=FALSE}
TPQQuestions = list(NS1=c(2, 4, 9, 11, 40, 43, 85, 93, 96),
                    NS2=c(30, 46, 48, 50, 55, 56, 81, 99),
                    NS3=c(32, 66, 70, 72, 76, 78, 87),
                    NS4=c(13, 16, 21, 22, 24, 28, 35, 60, 62, 65),
                    HA1=c(1, 5, 8, 10, 14, 82, 84, 91, 95, 98),
                    HA2=c(18, 19, 23, 26, 29, 47, 51),
                    HA3=c(33, 37, 38, 42, 44, 89, 100),
                    HA4=c(49, 54, 57, 59, 63, 68, 69, 73, 75, 80),
                    RD1=c(27, 31, 34, 83, 94),
                    RD2=c(39, 41, 45, 52, 53, 77, 79, 92, 97),
                    RD3=c(3, 6, 7, 12, 15, 64, 67, 74, 86, 88, 90),
                    RD4=c(17, 20, 25, 36, 58),
                    NS=c(2, 4, 9, 11, 40, 43, 85, 93, 96, 30, 46, 48, 50, 55, 56, 81, 99, 32, 66, 70, 72, 76, 78, 87, 13, 16, 21, 22, 24, 28, 35, 60, 62, 65),
                    HA=c(1, 5, 8, 10, 14, 82, 84, 91, 95, 98, 18, 19, 23, 26, 29, 47, 51, 33, 37, 38, 42, 44, 89, 100, 49, 54, 57, 59, 63, 68, 69, 73, 75, 80),
                    RD=c(27, 31, 34, 83, 94, 39, 41, 45, 52, 53, 77, 79, 92, 97, 3, 6, 7, 12, 15, 64, 67, 74, 86, 88, 90, 17, 20, 25, 36, 58))


for (i in 1:length(weighted_noboot_fast_list)){
  for (n in 1:length(TPQQuestions)){
    item = TPQQuestions[[n]]
    i_name = names(TPQQuestions)[n]
    # calculate the main dimensions from the original scores
    if (i<6){
      weighted_fast_list[[i]][i_name] = scores[i_name]
      weighted_info_list[[i]][i_name] = scores[i_name]
    }
    weighted_noboot_fast_list[[i]][i_name] = scores[i_name]
      
    # create a weighted HA
    if (i<6){
      weighted_fast_list[[i]][paste0("weight_",i_name)] = apply(weighted_fast_list[[i]][item],1,sum, na.rm = TRUE)
      weighted_info_list[[i]][paste0("weight_",i_name)] = apply(weighted_info_list[[i]][item],1,sum, na.rm = TRUE)
    }
    weighted_noboot_fast_list[[i]][paste0("weight_",i_name)] = apply(weighted_noboot_fast_list[[i]][item],1,sum, na.rm = TRUE)
  }
}

calc_r <- function(var_name, data_frame){
  text_form = as.formula(paste0(var_name," ~ ",paste(colnames(weighted_fast)[TPQQuestions[[var_name]]],collapse = " + ")))
  r_original = round(100*summary(lm(text_form, data = scores))$adj.r.squared,1)
  r_weighted = round(100*summary(lm(text_form, data = data_frame))$adj.r.squared,1)
  text = paste0(var_name,": r2 for original scores =", r_original, "% ... r2 for weighted scores =", r_weighted,"%")
  print(text)
  return(r_weighted)
}


fast_ns = mean(sapply(1:length(weighted_fast_list),function(x) calc_r("NS", weighted_fast_list[[x]])))
fast_ha = mean(sapply(1:length(weighted_fast_list),function(x) calc_r("HA", weighted_fast_list[[x]])))
fast_rd = mean(sapply(1:length(weighted_fast_list),function(x) calc_r("RD", weighted_fast_list[[x]])))


noboot_fast_ns = mean(sapply(1:length(weighted_noboot_fast_list),function(x) calc_r("NS", weighted_noboot_fast_list[[x]])))
noboot_fast_ha = mean(sapply(1:length(weighted_noboot_fast_list),function(x) calc_r("HA", weighted_noboot_fast_list[[x]])))
noboot_fast_rd = mean(sapply(1:length(weighted_noboot_fast_list),function(x) calc_r("RD", weighted_noboot_fast_list[[x]])))


info_ns = mean(sapply(1:length(weighted_info_list),function(x) calc_r("NS", weighted_info_list[[x]])))
info_ha = mean(sapply(1:length(weighted_info_list),function(x) calc_r("HA", weighted_info_list[[x]])))
info_rd = mean(sapply(1:length(weighted_info_list),function(x) calc_r("RD", weighted_info_list[[x]])))

mm = matrix(c(fast_ns, fast_ha,fast_rd,
         noboot_fast_ns, noboot_fast_ha,noboot_fast_rd,
         info_ns, info_ha,info_rd), nrow = 3, byrow = TRUE)
colnames(mm)=c("NS","HA","RD")
rownames(mm) = c("fast_boot", "fast_noboot","infomax_boot")

```

```{r check binomiality, or the percentage of binomial ICs, warning=FALSE}
# create a new data frame to plot all densities
melt_data <- function(data_frame){
  # this function creates data list with D1 as the first digit of the question
  # and D2 as the second one (for plotting it as a table)
  # also, density is calculated in the middle of values, if it is >0.05 of max density, 
  # this variable will be considered a binomial-density variable
  Data = melt(data_frame,measure.vars = c(1:100), id.vars = "ID")
  Data$D1 = rep(0:9,each = 18190)
  Data$D2 = rep(rep(1:10,each = 1819),10)
  for (ll in levels(Data$variable)){
    dd = density(Data$value[Data$variable == ll], na.rm = TRUE)
    Data$binomial[Data$variable == ll] = ifelse(dd$y[dd$x==min(dd$x[dd$x>0])]/max(dd$y) > 0.05, "","B")
  }
  #only the last item should be present (so that when I plot, only one character is plotted, 
  # rather than all being plotted 1819 times over each other)
  Data$binomial[(rep(1:181900)/1819)%%1 != 0] = ""
  binomialcount = sum(Data$binomial[(rep(1:181900)/1819)%%1 == 0] == "B")
  Data$binomial[Data$binomial=="B"] = paste0("B",binomialcount)
  return(list(Data,binomialcount))
}



ggplot(data = melt_data(weighted_fast_list[[1]])[[1]],mapping = aes(x=value))+
  geom_density()+
  facet_grid(D1~D2,scales = "free_y")+
  geom_text(mapping = aes(y=10,x=-0.5,label=binomial, color = "red"))

bi_fast_count = append(sapply(1:length(weighted_fast_list), function(x) melt_data(weighted_fast_list[[x]])[[2]]),rep(0,9))
bi_noboot_fast_count = sapply(1:length(weighted_noboot_fast_list), function(x) melt_data(weighted_noboot_fast_list[[x]])[[2]])
bi_info_count = append(sapply(1:length(weighted_info_list), function(x) melt_data(weighted_info_list[[x]])[[2]]),rep(0,9))

m_bi = matrix(c(bi_fast_count, bi_noboot_fast_count, bi_info_count), nrow = 14,byrow=FALSE)
rownames(m_bi) = 1:14
colnames(m_bi) = c("Fast_boot","Fast_noboot","Infomax_boot")
d_bi = melt(data.frame(m_bi))
d_bi$IC_data_num = rep(1:14,3)
ggplot(data = d_bi, mapping = aes(x = IC_data_num, y=value, group = variable, fill=variable))+
  geom_bar(stat = "identity",position = position_dodge(0.9),color = "black")+
  scale_y_continuous(name = "Number of Binomial Questions")+
  scale_x_continuous(name = "IC #")+
  scale_fill_manual(values = c("#Eb4C42","#4EA3DF","#6cBE58"))+
  ggtitle("Binomial Question Count in all ICs in all IC Methods")+
  TypicalTheme
```


```{r check differences between diagnosis groups}


prepare_data_means <- function(var_name, data_frame){
  weighted_name = paste0("weight_",var_name)
  data_frame$Diagnosis = as.factor(data_frame$Diagnosis)
  data_frame$Diagnosis = relevel(data_frame$Diagnosis,ref = "HC")
  
  weight_ll = lm(weight_HA~Diagnosis, data = data_frame[complete.cases(data_frame[weighted_name]),])
  ll = lm(HA~Diagnosis, data = data_frame[complete.cases(data_frame$weight_HA),])

  Data1 = SMeans(data_frame, DV = weighted_name, IVs = c("Diagnosis"),GroupBy = "Diagnosis")
  Data2 = SMeans(data_frame, DV = var_name, IVs = c("Diagnosis"),GroupBy = "Diagnosis")
  Data1[var_name] = Data2[var_name]
  
  DataMeans = melt(Data1, measure.vars = c(var_name, weighted_name))
  
  return(list(DataMeans,summary(weight_ll),summary(ll)))
}

var = "RD"
DataFrame = weighted_info_list[[2]]
DataMeans_list = prepare_data_means(var_name = var,
                               data_frame = DataFrame)
DataMeans = DataMeans_list[[1]]
DataMeans_list[[2]]
DataMeans_list[[3]]
ggplot(DataMeans,aes(x=Diagnosis,y= value, fill=variable))+
  geom_bar(stat="identity",position = position_dodge(0.9),color="black")+
  TypicalTheme+
  scale_fill_manual(values = c("#C33E3B","#4EA3DF","#6cBE58","#1B4D3E","#003153"))+
  geom_errorbar(aes(ymax = value+SEM,ymin = value-SEM),width = 0.5, position = position_dodge(0.9))+
  ggtitle(paste0(var," and Wieghted-",var, " scores"),subtitle = "for Infomax Bootstrapping")

Correlate(weighted_info_list[[5]], Var1 = "weight_HA","HA", Factor = "Diagnosis")
```

```{python, check questions that make up each of the ICs}
colnames = {i:"Q"+str(i)+"_loading" for i in range(1,101)}
colnames[0]="ID"
for i in range(5):
  data_info[i] = data_info[i].rename(columns = colnames)
  data_fast[i] = data_fast[i].rename(columns = colnames)
  data_noboot_fast[i] = data_noboot_fast[i].rename(columns = colnames)

def find_questions(data_frame):
  w_data = data_frame.iloc[:,1:101]
  
  # make sure to take the absolute value
  w_data = abs(w_data.astype(float))
  
  # take the average of questions around subjects
  w_data_avg = np.average(w_data,axis=0)
  
  # threshold of 0.25 is applied
  questions_included = np.where(w_data_avg>0.25,1,0)
  return(questions_included.tolist())

questions_included = pd.DataFrame([find_questions(data_info[i]) for i in range(5)])

plt.scatter(range(0,100),questions_included.iloc[0,:])
plt.scatter(range(0,100),questions_included.iloc[1,:]*2)
plt.scatter(range(0,100),questions_included.iloc[2,:]*3)
plt.scatter(range(0,100),questions_included.iloc[3,:]*4)
plt.scatter(range(0,100),questions_included.iloc[4,:]*5)
plt.savefig("/home/asawalma/questions.png", dpi = 200)
```

```{r, warning=FALSE}
# create a data frame of IC loadings
data_info_average = list()
data_info_average_abs = list()
for (i in 1:length(data_info)){
  data_info_average[[i]] = apply(data_info[[i]][,c(2:101)],2,mean)
  data_info_average_abs[[i]] = apply(abs(data_info[[i]][,c(2:101)]),2,mean)
}

#change names to reflect the IC they came from
names(data_info_average) = sapply(0:4, function(x) paste0("IC",x))
names(data_info_average_abs) = sapply(0:4, function(x) paste0("IC",x))

# make the list into data frame. There will be two columns the first is the IC, the second is the loading
data_info_avg = melt(data.frame(data_info_average),variable.name = "IC", value.name = "Loading")
data_info_avg_abs = melt(data.frame(data_info_average_abs),variable.name = "IC", value.name = "Loading")
data_info_avg$Loading_abs = data_info_avg_abs$Loading

# Add a column to represent questions
data_info_avg$Question = rep(1:100,5)

ggplot(data = data_info_avg,aes(x = Question, y = Loading, fill = IC))+
  geom_point(shape = 21, color = "black",size = 4)+
  TypicalTheme

# find included questions, if they are above threshold, give a value of 1, otherwise give 0
data_info_avg$Inclusion = ifelse(data_info_avg$Loading>0.25, 1, 0)
data_info_avg$Inclusion_abs = ifelse(data_info_avg$Loading_abs>0.25, 1, 0)

sum(apply(sapply(c("IC1","IC2","IC3"), function(x) data_info_avg$Inclusion_abs[data_info_avg$IC == x]),1,sum)==1)


ggplot(data = data_info_avg,aes(x = Question, y = Loading_abs, fill = IC))+
  geom_point(shape = 21, color = "black",size = 3)+
  TypicalTheme


sapply(c("IC0","IC1","IC2","IC3","IC4"), function(x) data_info_avg$Inclusion_abs[data_info_avg$IC == x])




```

```{r check how much each subject have loadings similar to Cloninger Loadings, warning=FALSE}
TPQQuestions_list = list(NS1=c(2, 4, 9, 11, 40, 43, 85, 93, 96),
                    NS2=c(30, 46, 48, 50, 55, 56, 81, 99),
                    NS3=c(32, 66, 70, 72, 76, 78, 87),
                    NS4=c(13, 16, 21, 22, 24, 28, 35, 60, 62, 65),
                    HA1=c(1, 5, 8, 10, 14, 82, 84, 91, 95, 98),
                    HA2=c(18, 19, 23, 26, 29, 47, 51),
                    HA3=c(33, 37, 38, 42, 44, 89, 100),
                    HA4=c(49, 54, 57, 59, 63, 68, 69, 73, 75, 80),
                    RD1=c(27, 31, 34, 83, 94),
                    RD2=c(39, 41, 45, 52, 53, 77, 79, 92, 97),
                    RD3=c(3, 6, 7, 12, 15, 64, 67, 74, 86, 88, 90),
                    RD4=c(17, 20, 25, 36, 58),
                    NS=c(2, 4, 9, 11, 40, 43, 85, 93, 96, 30, 46, 48, 50, 55, 56, 81, 99, 32, 66, 70, 72, 76, 78, 87, 13, 16, 21, 22, 24, 28, 35, 60, 62, 65),
                    HA=c(1, 5, 8, 10, 14, 82, 84, 91, 95, 98, 18, 19, 23, 26, 29, 47, 51, 33, 37, 38, 42, 44, 89, 100, 49, 54, 57, 59, 63, 68, 69, 73, 75, 80),
                    RD=c(27, 31, 34, 83, 94, 39, 41, 45, 52, 53, 77, 79, 92, 97, 3, 6, 7, 12, 15, 64, 67, 74, 86, 88, 90, 17, 20, 25, 36, 58))


# convert this list to lists of 100 questions, with ones in place of question numbers
TPQQuestions_list = sapply(TPQQuestions_list, function(x) ifelse(1:100 %in% x,1,0))

IC_num = 3
Threshold = 0.25
data_frame = data_fast
# Choose data frame (without the column "arrangement")
IC_threshold = data.frame(data_frame[[IC_num]][,-c(102)])
IC_threshold[,c(2:101)] = abs(IC_threshold[,2:101]) > Threshold
colnames(IC_threshold) = c("ID",1:100, "Diagnosis")
IC_threshold$ID = 1:1819

# change True/False to 1/0
IC_threshold[,c(2:101)] = apply(IC_threshold[,c(2:101)],2,ifelse,1,0)

IC_threshold_melt = melt(IC_threshold, value.name = "Threshold", variable.name =  c("Question"), id.vars = c("ID","Diagnosis"))

ggplot(IC_threshold_melt, aes(x = ID, y = Question)) + 
  geom_raster(aes(fill=Threshold)) + 
  scale_fill_gradient(low="white", high="#D40000") +
  ggtitle(paste0("Questions That Pass the Threshold of ",Threshold," in IC ", IC_num)) +
  TypicalTheme + 
  theme(axis.text.x = element_text(angle = 90))


# Add Cloninger loadings
TPQQuestions = data.frame(t(TPQQuestions_list))
colnames (TPQQuestions) = c(1:100)
TPQQuestions$ID = rownames(TPQQuestions)
TPQQuestions$Diagnosis = "Cloninger"

# reorder columns to match that of IC_threshold
TPQQuestions = TPQQuestions[c(101,1:100,102)]

# create a new data frame that has the resemblance values for each subject
Cloninger_perc = rbind(IC_threshold,TPQQuestions)

#create a function to determine the loading similarity for all TPQgroups



for (c_item in 1:length(colnames(TPQQuestions_list))){
  percentage = apply(abs(Cloninger_perc[,c(2:101)] - TPQQuestions_list[,c_item]),1,sum,na.rm = TRUE)
  Cloninger_perc[,colnames(TPQQuestions_list)[c_item]] = percentage
}

Cloninger_perc_melt = melt(Cloninger_perc[,c(1,102:117)], value.name = "Threshold", variable.name =  c("Dimension"), id.vars = c("ID","Diagnosis"))
Cloninger_perc_melt$ID = as.numeric(Cloninger_perc_melt$ID)

ggplot(Cloninger_perc_melt, aes(x = ID, y = Dimension)) + 
  geom_raster(aes(fill=Threshold)) + 
  scale_fill_gradient2(low="white", high="blue", midpoint = 40) +
  ggtitle(paste0("Questions That Pass the Threshold of ",Threshold," in IC ", IC_num)) +
  TypicalTheme + 
  theme(axis.text.x = element_text(angle = 90))

sapply(c("IC0","IC1","IC2","IC3","IC4"), function(x) IC_threshold$Threshold[IC_threshold$IC == x])

```

```{r 3d Matrix for all loadings, for 5 subject groups, warning=FALSE, cache=TRUE}

# define column names to be included for the loadings and those for the weighted final scores
weighted_col_list = sapply(1:100, function(x) paste0("QO",x))
loading_col_list = sapply(1:100, function(x) paste0("Q",x,"_load"))

# define a function to plot loadings
plot_questions <- function(question_df, plotting = "loadings") {
  # plotting can be set to "loadings", "scores" or "both"
  
  # force the levels of "ID" to follow the diagnosis (make sure they are not divided into two sessions)
  # First, find the IDs that represent each of the Diagnoses
  ID_list = sapply(1:5, function(x)
    question_df$ID[question_df$Diagnosis == levels(factor(question_df$Diagnosis))[x]])
  
  #extract the length of each item so that it can be plotted as a horizontal line to separate each group
  Diagnoses_positions = sapply(1:5, function (x)
    sum(sapply(ID_list[1:x], function(y)
      length(y))))
  
  #Change the ID into a factor, make sure the factor levels separate the data into 5 Diagnosis groups
  question_df$ID = factor(question_df$ID, levels = unlist(ID_list))
  question_df = Multimelt(
    question_df,
    weighted_col_list,
    loading_col_list,
    Names = c("score", "loading"),
    new.factor = "Question"
  )
  
  # check each question for the following condition:
  # dt = double_threshold: if the count of the above-threshold loadings is more than 80% for that question,
  # then this question is included
  included_loadings_dt = sapply(1:100, function(x)
    sum(abs(question_df$loading[question_df$Question == weighted_col_list[x]]) > 0.25, na.rm = TRUE) > (0.8 * 1819))
  included_scores_dt = sapply(1:100, function(x)
    sum(abs(question_df$score[question_df$Question == weighted_col_list[x]]) > 0.25, na.rm = TRUE) > (0.8 * 1819)) 

  # av = average: if the average loading is >0.25, this question is included
  averages_loadings = sapply(1:100, function(x)
                    mean(question_df$loading[question_df$Question == weighted_col_list[x]],na.rm = TRUE))
  included_loadings_av = abs(averages_loadings)>0.25
  averages_scores = sapply(1:100, function(x)
                    mean(question_df$score[question_df$Question == weighted_col_list[x]],na.rm = TRUE))
  included_scores_av = abs(averages_scores)>0.25
  
  #plot loadings
  load_plot = ggplot(question_df, aes(x= Question, y = ID))+
    geom_raster(aes(fill=loading)) + 
    scale_fill_gradient2(high="#CC0000", low="#08457E", mid = "white", midpoint = 0, limits = c(-1,1)) +
    ggtitle(label = "Final Loadings for All Subjects", subtitle = "For the 5 Groups of Subjects")+
    geom_hline(yintercept=Diagnoses_positions, size=0.5)+
    annotate(geom="text", x=-3.5, y=c(Diagnoses_positions-15, -15, 1845, 1885),
             label=c("GAD","HC","MDD","PTSD","TNP", "Ex. Questions","DT. Questions","AVG. Questions"), size = 4)+
    coord_cartesian(ylim =c(-30,1900), xlim = c(-7,100))+
    geom_point(data = data.frame(Inc_q = included_loadings_dt*1860-20),mapping = aes(x=1:100, y = Inc_q), 
               size = 2, color = "#1B4D3E", inherit.aes = FALSE)+
    geom_point(data = data.frame(Inc_q = included_loadings_av*1900-20),mapping = aes(x=1:100, y = Inc_q), 
           size = 2, color = "#6cBE58", inherit.aes = FALSE)+
    TypicalTheme
  
  #plot scores
  score_plot = ggplot(question_df, aes(x= Question, y = ID))+
    geom_raster(aes(fill=score)) + 
    scale_fill_gradient2(high="#CC0000", low="#08457E", mid = "white", midpoint = 0, limits = c(-1,1)) +
    ggtitle(label = "Final Scores for All Subjects", subtitle = "For the 5 Groups of Subjects")+
    geom_hline(yintercept=Diagnoses_positions, size=0.5)+
    annotate(geom="text", x=-3.5, y=c(Diagnoses_positions-15, -15, 1845, 1885),
             label=c("GAD","HC","MDD","PTSD","TNP", "Ex. Questions","DT. Questions","AVG. Questions"), size = 4)+
    coord_cartesian(ylim =c(-30,1900), xlim = c(-7,100))+
    geom_point(data = data.frame(Inc_q = included_scores_dt*1860-20),mapping = aes(x=1:100, y = Inc_q), 
               size = 2, color = "#1B4D3E", inherit.aes = FALSE)+
    geom_point(data = data.frame(Inc_q = included_scores_av*1900-20),mapping = aes(x=1:100, y = Inc_q), 
           size = 2, color = "#6cBE58", inherit.aes = FALSE)+
    TypicalTheme
  
  # return the both figures if mult_plot is set to TRUE. Otherwise, return loadings only
  if (plotting == "loadings"){
    return (load_plot)
  }else if (plotting == "scores"){
    return (score_plot)
  }else if(plotting == "both"){
    return(multiplot(load_plot,score_plot,cols = 2))
  }else{
    return("Please set a valid argument for plotting, it can be set to 'loadings', 'scores' or 'both'")
  }
}

fig_dir = "/home/asawalma/git/TPQ/figures"
if (!dir.exists(fig_dir)) {
  fig_dir = "/home/abdulrahman/git/TPQ/figures"
}

#save all figures for each of the IC data frames
sapply(1: length(df_fast_list), function(x)
  ggsave(paste0(fig_dir,"/Q_Threshold_FastICA_Boot_",x,".png"),
    plot = plot_questions(df_fast_list[[x]], plotting = "loadings"), width = 570,
    height = 340, units = "mm", limitsize = FALSE))

sapply(1: length(df_noboot_fast_list), function(x)
  ggsave(paste0(fig_dir,"/Q_Threshold_FastICA_NoBoot_",x,".png"),
    plot = plot_questions(df_noboot_fast_list[[x]], plotting = "loadings"), width = 570,
    height = 340, units = "mm", limitsize = FALSE))

#save all figures for each of the IC data frames
sapply(1: length(df_info_list), function(x)
  ggsave(paste0(fig_dir,"/Q_Threshold_Infomax_Boot_",x,".png"),
    plot = plot_questions(df_info_list[[x]], plotting = "loadings"), width = 570,
    height = 340, units = "mm", limitsize = FALSE))
```

```{r similarity matrix, warning=FALSE, cache = TRUE}
# define column names to be included for the loadings and those for the weighted final scores
weighted_col_list = sapply(1:100, function(x) paste0("QO",x))
loading_col_list = sapply(1:100, function(x) paste0("Q",x,"_load"))

cor_fun <- function(df, type = "loadings"){
  if (type == "loadings"){
    questions_inc = loading_col_list
  } else if (type == "scores"){
    questions_inc = weighted_col_list
  }
  cor_df = data.frame(cor(df[questions_inc],use="pairwise.complete.obs"))

  cor_df$Q1 = sapply(1:100, function(x) paste0("Q",x))
  
  cor_df_melt = melt(cor_df, measure.vars = questions_inc, variable.name = "Q2", value.name = "Correlation")
  cor_df_melt$Q2 = rep(sapply(1:100, function(x) paste0("Q",x)),each = 100)
  
  g_cor = ggplot(cor_df_melt, mapping = aes(x=Q1, y=Q2))+
    geom_raster(aes(fill=Correlation)) + 
    scale_fill_gradient2(high="#CC0000", low="#08457E", mid = "white", midpoint = 0, limits = c(-1,1))

    return (g_cor)
}
  
fig_dir = "/home/asawalma/git/TPQ/figures"
if (!dir.exists(fig_dir)) {
  fig_dir = "/home/abdulrahman/git/TPQ/figures"
}

sapply(1:length(df_fast_list), function(x)
  ggsave(paste0(fig_dir,"/Cor_FastICA_Boot",x,".png"), plot = cor_fun(df_fast_list[[x]]),
       width = 250, height = 250, units = "mm", limitsize = FALSE))

sapply(1:length(df_noboot_fast_list), function(x)
  ggsave(paste0(fig_dir,"/Cor_FastICA_NoBoot",x,".png"), plot = cor_fun(df_noboot_fast_list[[x]]),
       width = 250, height = 250, units = "mm", limitsize = FALSE))

sapply(1:length(df_info_list), function(x)
  ggsave(paste0(fig_dir,"/Cor_Infomax",x,".png"), plot = cor_fun(df_info_list[[x]]),
       width = 250, height = 250, units = "mm", limitsize = FALSE))

```

```{r trials, warning=FALSE}
#if(require(MASS)){
library(MASS)
library(fastICA)
x <- mvrnorm(n = 1000, mu = c(0, 0), Sigma = matrix(c(10, 3, 3, 1), 2, 2))
x1 <- mvrnorm(n = 1000, mu = c(-1, 2), Sigma = matrix(c(10, 3, 3, 1), 2, 2))
X <- rbind(x, x1)
a <- fastICA(X, 2, alg.typ = "deflation", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
par(mfrow = c(1, 3))
plot(x, main = "Pre-processed data")
plot(a$X, main = "Pre-processed data")
plot(a$X %*% a$K, main = "PCA components")
plot(a$S, main = "ICA components")

fastICA(scores[141:240], 2, alg.typ = "deflation", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)

```


```{python trials}

```